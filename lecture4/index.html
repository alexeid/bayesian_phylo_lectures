---
lecture_num: 4

reveal:
    history: true
    slideNumber: true
---

<section>
    <section class="center">
        <h2>Hidden Markov Models (HMMs)</h2>
    </section>

    <section>
        <h3>Markov Chains</h3>

        <ul class="spaced" style="margin-top:2em">
            <li class="fragment">Markov Chains are discrete stochastic processes which obey the Markov property:

                <blockquote>
                    $$P(X_{i}|X_{i-1},X_{i-2},\ldots,X_0) = P(X_i|X_{i-1})$$
                </blockquote>
            </li>

            <li class="fragment">The index $i$ is often interpreted as "time", but it need not be.</li>

            <li class="fragment">Homogeneous Markov chains satisfy the additional requirement:
                <blockquote>
                    $$P(X_{i+1}=y|X_{i}=x)=P(X_i=y|X_{i-1}=x)$$
                </blockquote>
            </li>
            <li class="fragment">This allows the vector $\vec{P}_i$ of probabilities at $i$ to be expressed as
                $$\vec{P}_i=M\vec{P}_{i-1}=M^i\vec{P}_{0}$$
                where $M$ is the matrix of <b>transition probabilities</b>.</li>
        </ul>

    </section>

    <section>
        <h3>Markov Chains as Finite State Machines</h3>

        <p>Homogeneous Markov chains can be represented as finite state machines.</p>

        <img data-src="DNA_FSM.svg" style="width:50%">

        <p>Arrows can be labeled with transition probabilities to complete the description.</p>
    </section>

    <section>
        <h3>Modelling begin and end states</h3>

        <p>Including the begin and end states explicitly allows the length of
        the chain to be modelled.</p>

        <img data-src="DNA_FSM_BE.svg" style="width:60%">

        <p>If the transition probabilities to the end state are uniform, i.e.
        $M_{x\mathcal{E}}=p~~\forall x$ then the length has a geometric distribution:</p>
        $$P(L=l)=P(X_{l+1}=\mathcal{E})=(1-p)^{l}p$$
    </section>

    <section>
        <h3>Markov Chains with hidden state</h3>

        <ul class="spaced" style="margin-top:10%">
            <li>For HMMs, observations are tied to states probabilistically.</li>
            <li class="fragment">Consider $Y_1,Y_2,\ldots,Y_N$ to be the sequence of hidden states
                evolving according to the transition probability matrix $M$.</li>
            <li class="fragment">Observations $X_1,X_2,\ldots,X_N$ are each drawn from $P(X_i|Y_i)$.</li>
            <li class="fragment">Assuming homogeneity lets us define
                $$P(X_i=x|Y_i=y) =e_y(x)$$
                to be the <b>emission probability</b>.</li>
        </ul>
    </section>

    <section>
        <h3>HMM example: occasionally dishonest casino</h3>

        <img data-src="HT_HMM.svg" style="width:60%">

        <div class="textbox">
            <div class="title"><a id="HMMsim" href="#" style="color:white">Run Demo</a></div>
            <pre id="HMMsim_output">

            </pre>
        </div>
    </section>
</section>

<section>
    <section class="center">
        <h2>Finding optimal paths</h2>
    </section>

    <section>
        <h3>Most probable paths</h3>

        <p>The most probable path $\vec{Y}$ through the hidden state space is:</p>

        \begin{align*}
        \vec{Y}^* &amp;= \underset{\vec{Y}}{\text{argmax}}~P(\vec{Y}|\vec{X})\\
        &amp; = \underset{\vec{Y}}{\text{argmax}}~\frac{P(\vec{Y},\vec{X})}{P(\vec{X})}\\
        &amp; = \underset{\vec{Y}}{\text{argmax}}~P(\vec{Y},\vec{X})
        \end{align*}

        <ul>
            <li class="fragment">Question: Many possible paths!! How do we find the best one?</li>
            <li class="fragment">Answer: Dynamic Programming!</li>
        </ul>

        <p class="fragment">Define $v_k(i)$ as the joint probability of the most probable (sub) path ending with
        observation number $i$ in state $k$.  If this is known for all states $k$, can compute:
        $$v_l(i+1) = e_l(x_{i+1})\max_{k}(v_k(i)M_{kl})$$</p>

        <blockquote class="fragment">
            Optimal substructure!
        </blockquote>
    </section>

    <section>
        <h3>Why "Dynamic Programming"?</h3>

        <blockquote style="width:90%; margin-top: 2em; font-size:0.7em">
            The 1950s were not good years for mathematical research. We
            had a very interesting gentleman in Washington named Wilson. He was
            Secretary of Defense, and he actually had a pathological fear and
            hatred of the word research. I’m not using the term lightly; I’m using
            it precisely. His face would suffuse, he would turn red, and he would
            get violent if people used the term research in his presence. You can
            imagine how he felt, then, about the term mathematical. The RAND
            Corporation was employed by the Air Force, and the Air Force had Wilson
            as its boss, essentially. Hence, I felt I had to do something to shield
            Wilson and the Air Force from the fact that I was really doing
            mathematics inside the RAND Corporation. What title, what name, could I
            choose? In the first place I was interested in planning, in decision
            making, in thinking. But planning, is not a good word for various
            reasons. I decided therefore to use the word “programming”. I wanted to
            get across the idea that this was dynamic, this was multistage, this
            was time-varying. I thought, let's kill two birds with one stone. Let's
            take a word that has an absolutely precise meaning, namely dynamic, in
            the classical physical sense. It also has a very interesting property
            as an adjective, and that it's impossible to use the word dynamic in a
            pejorative sense. Try thinking of some combination that will possibly
            give it a pejorative meaning. It's impossible. Thus, I thought dynamic
            programming was a good name. It was something not even a Congressman
            could object to.

            <div class="cite">Richard Bellman, "Eye of the Hurricane: An Autobiography", 1984</div>
        </blockquote>
    </section>

    <section>
        <h3>Viterbi Algorithm</h3>

        <div class="textbox" style="width:70%;margin-left:15%;margin-top:10%">
            <dl>
                <dt>Initialization ($i=0$):</dt><dd>
                $v_{k}(0) = \delta_{k,k_0}$ where $k_0=\underset{k}{\text{argmax}}~P(Y_0=k)$
                </dd>
                <dt>Recursion ($i = 1\ldots L$):</dt><dd></dd>
                $$v_l(i) = e_l(x_i)\max_{k}(v_k(i-1)M_{kl})$$
                $$\text{ptr}_i(l) = \underset{k}{\text{argmax}}(v_k(i-1)M_{kl})$$
                <dt>Termination:</dt><dd>
                $$P(\vec{X},\vec{Y}^*) = \max_k(v_k(L))$$
                $$Y^*_L = \underset{k}{\text{argmax}}(v_k(L))$$
                </dd>
                <dt>Traceback ($i = L\ldots 1$):</dt><dd>
                $$Y^*_{i-1} = \text{ptr}_i(Y^*_i)$$
                </dd>
            </dl>
        </div>

        <p>(Optimal path through network whiteboard example.)</p>

    </section>

    <section>
        <h3>Detecting coin fairness</h3>

        <div class="textbox" style="margin-top:20%">
            <div class="title"><a id="HMMviterbi" href="#" style="color:white">Run Viterbi Demo</a></div>
            <pre id="HMMviterbi_output">



            </pre>
        </div>
    </section>
</section>

<section>
    <section class="center">
        <h2>Computing Probabilities</h2>
    </section>

    <section>
        <h2>Forward Algorithm</h2>

        <p>This algorithm computes the probability of the sequence of observations given the model.<br>
        It is trivial to derive.</p>

        <p class="fragment">Define $f_k(i) = P(X_1,\ldots,X_i,Y_i=k)$.</p>

        <div class="textbox fragment" style="width:70%;margin-left:15%;">
            <dl class="spaced">
                <dt>Initialization</dt><dd> $$f_k(0) = P(Y_0=k)$$ </dd>
                <dt>Recursion ($i=1\ldots L$)</dt>
                <dd>$$f_l(i) = e_l(x_i)\sum_k f_k(i-1)M_{kl}$$</dd>
                <dt>Termination</dt>
                <dd>$P(\vec{X})=\sum_k f_k(L)$</dd>
            </dl>
        </div>

    </section>

    <section>
        <h2>Posterior state probabilities</h2>

        <p>It is interesting to know the posterior probability of hidden states at each site.</p>
        <blockquote class="fragment">
            This is <b>NOT</b> the posterior distribution over paths. Stringing together the
            most probable states may not even produce a valid path!
        </blockquote>

        <p class="fragment">Start by considering:
        \begin{align*}
        P(\vec{X}, Y_i=k) &amp;= P(X_1,\ldots,X_i,Y_i=k)P(X_{i+1},\ldots,X_L|X_1,\ldots,X_{i},Y_i=k)\\
        &amp; = P(X_1,\ldots,X_i,Y_i=k)P(X_{i+1},\ldots,X_L|Y_i=k)\\
        &amp; \equiv f_k(i) b_k(i)
        \end{align*}</p>

        <p class="fragment">Use a similar recursion to the forward algorithm to find $b_k(i)$.</p>

        <p class="fragment">Then $$P(Y_i=k|\vec{X}) = \frac{f_k(i)b_k(i)}{P(\vec{X})}$$ where
        $P(\vec{X})$ is computed using either the forward or backward
        algorithms.</p>
    </section>

    <section>
        <h2>Backward Algorithm</h2>

        <p>This algorithm implements the recursion required to compute $b_k(i)$</p>

        <p class="fragment">Define $b_k(i) = P(X_{i+1},\ldots,X_L|Y_i=k)$.</p>

        <div class="textbox fragment" style="width:70%;margin-left:15%;">
            <dl class="spaced">
                <dt>Initialization</dt><dd> $$b_k(L+1) = 1$$</dd>
                <dt>Recursion ($i=L-1\ldots 1$)</dt>
                <dd>$$b_l(i) = \sum_k M_{lk}e_k(x_{i+1})b_k(i+1)$$</dd>
                <dt>Termination</dt>
                <dd>$P(\vec{X})=\sum_k P(Y_1=k)e_k(x_1)b_k(1)$</dd>
            </dl>
        </div>
    </section>

    <section>
        <h2>Posterior fairness probabilities</h2>

        <div class="textbox" style="margin-top:2em">
            <div class="title"><a id="HMMforwardBackward" href="#" style="color:white">Run Forward-Backward Demo</a></div>
            <pre id="HMMforwardBackwardOutput">
























            </pre>
        </div>

    </section>
</section>

<section>
    <section class="center">
        <h2>Parameter inference</h2>
    </section>

    <section>
        <h3>Known state trajectory</h3>

        <p style="margin-top:10%">When the hidden state trajectory $\vec{Y}$ is known, can use the
        following estimators for the transition and emission probabilities:

        $$M_{kl} = \frac{A_{kl}}{\sum_{l'}A_{kl'}}$$

        where $\mathcal{A}_{kl}$ are the number of transitions between states
        $k$ and $l$ on the trajectory.</p>

        <p class="fragment">Similarly,
        $$e_{k}(x) = \frac{E_{k}(x)}{\sum_{k'}E_{k'}(x)}$$
        where $E_k(x)$ is the number of emissions of $x$ from state $k$.</p>

        <p class="fragment">These are maximum likelihood (point) estimates. Can use <i>pseudo-count</i> offsets to
        avoid problems due to overfitting and insufficient data.</p>
        
    </section>

    <section>
        <h3>Unknown hidden states: Baum-Welch Algorithm</h3>

        <div class="textbox">
            <dl>
                <dt>Initialization</dt><dd>Pick arbitrary model parameters.</dd>
                <dt>Iterate</dt><dd>
                <ol>
                    <li>Set $A$ and $E$ to their pseudo-count values.</li>
                    <li>For each training sequence $\vec{X}^j$:
                        <ol>
                            <li>Calculate $f_k(i)$ for $\vec{X}^j$ using forward algorithm</li>
                            <li>Calculate $b_k(i)$ for $\vec{X}^j$ using backward algorithm</li>
                            <li>Add the contribution of $j$ to $A$ and $E$ using equations below.</li>
                        </ol>
                    </li>
                    <li>Update values for $M$ and $e$ based on ML estimates.</li>
                    <li>Compute likelihood of new model.</li>
                    <li>If change in likelihood is "small enough", stop.</li>
                </dd>
            </dl>

            <br>

        $$\langle A_{kl}\rangle = \sum_j\frac{1}{P(\vec{X}^j)}\sum_i f_k^j(i)M_{kl}e_l(x^{j}_{i+1})b^j_l(i+1)$$

        $$\langle E_k(x)\rangle = \sum_j\frac{1}{P(\vec{X}^j)}\sum_{i \text{ s.t. } X^j_i=x}f_k^j(i)b_k^j(i)$$

        </div>

    </section>
</section>

<section>
    <section class="center">
        <h2>HMMs in Bioinformatics</h2>
    </section>

    <section>
        <h3>Pairwise alignment with HMMs</h3>

        <p>The following FSM describes an HMM that generates pairs of aligned sequences:</p>
        <img data-src="pair_HMM.svg" style="width:80%">
    </section>

    <section>
        <h3>Most likely pairwise alignment</h3>

        <div class="textbox">
            <div class="title">Viterbi algorithm for pair HMMs</div>

            <dl class="spaced">
                <dt>Initialization</dt><dd>
                $v_M(0,0)=1$, $v_X(0,0) = v_Y(0,0) = 0$<br>
                $v_k(i,-1)=v_k(-1,j)=v_k(i,0)=v_k(0,j)=0$
                </dd>
                <dt>Recurrence:</dt><dd>
                \begin{align*}
                v_M(i,j) &amp;= p_{x_iy_j}\max\left\{\begin{array}{l}
                (1-2\delta-\tau)v_M(i-1,j-1)\\
                (1-\epsilon-\tau)v_X(i-1,j-1)\\
                (1-\epsilon-\tau)v_Y(i-1,j-1)
                \end{array}\right.\\

                v_X(i,j) &amp; = q_{x_i}\max\left\{\begin{array}{l}
                \delta v_M(i-1,j)\\
                \epsilon v_X(i-1,j);
                \end{array}\right.\\

                v_Y(i,j) &amp; = q_{y_i}\max\left\{\begin{array}{l}
                \delta v_M(i,j-1)\\
                \epsilon v_X(i,j-1);
                \end{array}\right.

                \end{align*}
                </dd>
                <dt>Termination:</dt><dd>
                $v_E=\tau\max(v_M(n,m), v_X(n,m), v_Y(n,m))$
                </dd>
            </dl>
        </div>
    </section>

    <section>
        <h3>Log-odds formulation</h3>

        <ul style="width:70%;margin-top:5%">
            <li class="fragment">Can produce (more!) exact analogues of the dynamic programming algorithms we saw earlier.</li>
            <li class="fragment">Needleman &amp; Wunch essentially produces a Viterbi path
            through a pair HMM with the following correspondences:</li>
        </ul>

        <br>
        <br>

        <div class="fragment">
        \begin{align*}
        s(a,b) &amp;= \log\frac{p_{ab}}{q_aq_b} + \log\frac{(1-2\delta-\tau)}{(1-\eta)^2}\\
        d &amp;= -\log\frac{\delta(1-\epsilon-\tau)}{(1-\eta)(1-2\delta-\tau)}\\
        e &amp;= -\log\frac{\epsilon}{1-\eta}
        \end{align*}

        <br>

        Here $\eta$ is the parameter determining the length of the alignment under the null model
        without the assumption of homology, used to construct log-odds values.)</div>
    </section>

    <section>
        <h3>Pairwise alignment probabilities</h3>

        <ul style="width:80%;margin-top:2em">
            <li class="fragment">Pair HMMs aren't just useful for rationalizing Needleman &amp; Wunch.</li>
            <li class="fragment">They allow us to compute the overall probability that sequences are related by <b>any</b> alignment:
                $$P(\vec{x},\vec{y}) = \sum_{\text{alignments }\alpha}P(\vec{x},\vec{y},\alpha)$$
            </li>
            <li class="fragment">Calculate this sum using the <b>forward algorithm</b>.</li>
            <li class="fragment">Can use this to evaluate the posterior probability that a given alignment is correct:
                $$P(\alpha|\vec{x},\vec{y})=\frac{P(\vec{x},\vec{y},\alpha)}{P(\vec{x},\vec{y})}$$
            </li>
            <li class="fragment">Can sample from this probability distribution!</li>

            <li class="fragment">Can also use the combined <b>forward</b> and <b>backward</b> algorithms to produce the probability that
                characters $x_i$ and $y_j$ are aligned:
                $$P(x_i\diamond y_j|\vec{x},\vec{y}) = \frac{f_M(i,j)b_M(i,j)}{P(\vec{x},\vec{y})}$$
            </li>
        </ul>
    </section>

    <section>
        <h3>Profile alignment with HMMs</h3>

        <p class="fragment">Use a single HMM to model an entire multiple sequence alignment:</p>
        <blockquote style="width:50%;text-align:center" class="fragment">
            <pre style="box-shadow:none;width:30%;font-size:1em;margin-top:0;margin-bottom:0;padding:0">
VGA--HAGEY
V----NVDEV
VEA--DVAGH
IAGADNGAGV </pre>
        </blockquote>
        <img class="fragment" data-src="profile_HMM.svg" style="width:60%"><br>

        <ul>
            <li class="fragment">Parameters can be estimated using Baum-Welch.</li>
            <li class="fragment">Once we have this <b>profile HMM</b> we can use it to search for other sequences which fit the
            profile and thus assess their candidacy for inclusion in the family.</li>
        </ul>
    </section>
</section>

<section>
    <h2>Summary</h2>

    <ol>
        <li class="fragment">Hidden Markov models (HMMs) model the stochastic generation of observations from a hidden state evolving according to a Markov chain.</li>
        <li class="fragment">Many useful algorithms exist for working with HMMs:
            <ul>
                <li class="fragment">The <b>Viterbi algorithm</b> for finding the optimal path,</li>
                <li class="fragment">the <b>forward algorithm</b> for finding the probability of the observations, and</li>
                <li class="fragment">the <b>backward algorithm</b> which allows the posterior probability of hidden states to be computed.</li>
            </ul>
        </li>
        <li class="fragment">Many applications of HMMs to bioinformatics:
            <ul>
                <li class="fragment">Pairwise alignment:
                    <ul>
                        <li class="fragment">Needleman-Wunch can be considered an application of Viterbi to a pair HMM</li>
                        <li class="fragment">Can compute alignment probabilities.</li>
                    </ul>
                </li>
                <li class="fragment">Profile alignment:
                    <ul>
                        <li class="fragment">Model MSA as an HMM.</li>
                        <li class="fragment">Estimate parameters using <b>Baum-Welch algorithm</b>.
                        <li class="fragment">Allows much more accurate classification of alignments than Feng-Doolittle.</li>
                        <li class="fragment">Implemented in Clustal$\Omega$.</li>
                </li>
            </ul>
        </li>

    </ol>
</section>

